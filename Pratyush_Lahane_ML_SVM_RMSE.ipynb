{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratyush-3000/me/blob/master/Pratyush_Lahane_ML_SVM_RMSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this assignment is to acquaint oneself with the SVM (Support Vector Machine) concept and gain practical experience in training both regression and classification SVM models."
      ],
      "metadata": {
        "id": "2__QNVdY0AZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 01"
      ],
      "metadata": {
        "id": "h7EzwM8J0AOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem you are going to work with \"Life Expectancy Data.csv\" dataset. <br>\n",
        "Import the dataset and\n",
        "1. Drop all the missing values from the dataset at the beginning of the project.\n",
        "2. Drop the \"Country\" column from the dataset.\n",
        "3. Split the dataset into train and test.\n",
        "4. Prepare the dataset using pipeline."
      ],
      "metadata": {
        "id": "o9lq0iF30AfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "df = pd.read_csv(\"Life Expectancy Data.csv\")\n",
        "\n",
        "df= df.dropna().drop(columns=\"Country\")\n",
        "\n",
        "x = df.drop(columns=\"Life expectancy\")\n",
        "y = df[\"Life expectancy\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "numeric_features = x.select_dtypes(include=['float64', 'int64']).columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features)\n",
        "    ])\n",
        "\n",
        "x_train_prepared = preprocessor.fit_transform(x_train)\n",
        "x_test_prepared = preprocessor.transform(x_test)\n",
        "\n",
        "(x_train_prepared.shape, x_test_prepared.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogc_zmTeyp4C",
        "outputId": "7306bdd7-2c71-4cd4-82af-aa3999c63009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1675, 17), (419, 17), (1675,), (419,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a linear SVM model to predict life expectancy. <br>\n",
        "Test the model on both train and test datasets and print out the RMSEs."
      ],
      "metadata": {
        "id": "SLcVHxV60Aim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "svm_model = SVR(kernel='linear')\n",
        "svm_model.fit(x_train_prepared, y_train)\n",
        "\n",
        "\n",
        "y_train_pred = svm_model.predict(x_train_prepared)\n",
        "y_test_pred = svm_model.predict(x_test_prepared)\n",
        "\n",
        "\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "rmse_train, rmse_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP8dQi3k1GrJ",
        "outputId": "f87d9466-b505-46af-9ce0-70b0fd4836e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.029706753853388, 4.090193442396489)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a second degree polynomial SMV, using kernel trick.\n",
        "Test the model on both train and test datasets and print out the RMSEs.\n",
        "Important points:\n",
        "1. The RMSEs of the polynomial model should be smaller than RMSEs of the linear model that you trained earlier. If they are not, find a way to solve the problem.\n",
        "2. Eliminate overfitting problem, **IF** you are overfitting."
      ],
      "metadata": {
        "id": "pKaFXWJZ1pvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "svm_poly_model_optimized = SVR(kernel='poly', degree=2, C=0.01, epsilon=0.1)\n",
        "svm_poly_model_optimized.fit(x_train_prepared, y_train)\n",
        "\n",
        "y_train_pred_poly_opt = svm_poly_model_optimized.predict(x_train_prepared)\n",
        "y_test_pred_poly_opt = svm_poly_model_optimized.predict(x_test_prepared)\n",
        "\n",
        "\n",
        "rmse_train_poly_opt = np.sqrt(mean_squared_error(y_train, y_train_pred_poly_opt))\n",
        "rmse_test_poly_opt = np.sqrt(mean_squared_error(y_test, y_test_pred_poly_opt))\n",
        "\n",
        "rmse_train_poly_opt, rmse_test_poly_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9YDXaT327tt",
        "outputId": "a242215c-c15a-4523-b039-aea77f4f44dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.819574924481083, 9.990382946594236)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 02"
      ],
      "metadata": {
        "id": "j8I9fpA02naQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem you are going to work with Iris dataset. <br>\n",
        "Import the dataset and\n",
        "1. **DO NOT** split the dataset into train and test; you should use the whole dataset as the trainset.\n",
        "2. Scale the dataset.\n",
        "3. Use all four inputs as the input of the project and all the classes as the output of the project (in class, I used two inputs and one class)."
      ],
      "metadata": {
        "id": "0IEOamEJ2qFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "\n",
        "x_scaled[:5], y[:5]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42-zHPr03YX3",
        "outputId": "60ffc809-5d70-4499-a607-f61be4a65029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
              "        [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ],\n",
              "        [-1.38535265,  0.32841405, -1.39706395, -1.3154443 ],\n",
              "        [-1.50652052,  0.09821729, -1.2833891 , -1.3154443 ],\n",
              "        [-1.02184904,  1.24920112, -1.34022653, -1.3154443 ]]),\n",
              " array([0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a linear SMV model and print out:\n",
        "1. Confusion matrix\n",
        "2. Precision score\n",
        "3. Recall score\n",
        "<br>\n",
        "\n",
        "**Important:** you need to train and test the model on the same dataset (as we did not split the dataset into train and test and there is no testset)."
      ],
      "metadata": {
        "id": "6JqJyuc43I-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(x_scaled, y)\n",
        "\n",
        "\n",
        "y_pred = svm_model.predict(x_scaled)\n",
        "\n",
        "conf_matrix = confusion_matrix(y, y_pred)\n",
        "precision = precision_score(y, y_pred, average='weighted')\n",
        "recall = recall_score(y, y_pred, average='weighted')\n",
        "\n",
        "conf_matrix, precision, recall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_NtNTyl35tz",
        "outputId": "20ca0ed5-60e0-4945-c516-d35edec3a415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[50,  0,  0],\n",
              "        [ 0, 46,  4],\n",
              "        [ 0,  1, 49]]),\n",
              " 0.9677505687140372,\n",
              " 0.9666666666666667)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a third degree polynomial model and print out:\n",
        "1. Confusion matrix\n",
        "2. precision score\n",
        "3. Recall score\n",
        "<br>\n",
        "\n",
        "**Important:** you need to train and test the model on the same dataset (as we did not split the dataset into train and test and there is no testset)."
      ],
      "metadata": {
        "id": "ue4lnwcV3s7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "svm_poly_model = SVC(kernel='poly', degree=3)\n",
        "svm_poly_model.fit(x_scaled, y)\n",
        "\n",
        "\n",
        "y_pred_poly = svm_poly_model.predict(x_scaled)\n",
        "\n",
        "conf_matrix_poly = confusion_matrix(y, y_pred_poly)\n",
        "precision_poly = precision_score(y, y_pred_poly, average='weighted')\n",
        "recall_poly = recall_score(y, y_pred_poly, average='weighted')\n",
        "\n",
        "conf_matrix_poly, precision_poly, recall_poly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ2i2osI4WGU",
        "outputId": "928d50f2-49e6-4d49-aa6a-92713797681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[50,  0,  0],\n",
              "        [ 0, 50,  0],\n",
              "        [ 0,  7, 43]]),\n",
              " 0.9590643274853801,\n",
              " 0.9533333333333334)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}